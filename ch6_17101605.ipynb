{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 학습 관련 기술들\n",
    "## 6.1 매개변수 갱신\n",
    "### 확률적 경사 하강법(SGD)<br> \n",
    "$$\\mathbf{W} \\leftarrow \\mathbf{W}-\\eta \\frac{\\partial L}{\\partial \\mathbf{W}}$$\n",
    "$\\mathbf{W}$ : 갱신할 가중치 매개변수<br>\n",
    "$\\frac{\\partial L}{\\partial \\mathbf{W}}$ : W에 대한 손실함수의 기울기<br>\n",
    "$\\eta$ : 학습률 (0.01이나 0.001과 같은 값을 미리 정해서 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD :\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr=lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr*grads[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ① 모멘텀\n",
    "$$\\mathbf{v} \\leftarrow \\alpha \\mathbf{v}-\\eta \\frac{\\partial L}{\\partial \\mathbf{W}}$$\n",
    "$$\\mathbf{W} \\leftarrow \\mathbf{W}+\\mathbf{v}$$\n",
    "$\\mathbf{v}$ : 속도<br>\n",
    "$\\alpha$ : 0.9 등의 값으로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum :\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr=lr\n",
    "        self.momentum=momentum\n",
    "        self.v=None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v={}\n",
    "            for key, val in params.items():\n",
    "                self.v[key]=np.zeros_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "                self.v[key]=self.momentum*self.v[key]-self.lr*grads[key]\n",
    "                params[key]+=self.v[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ② AdaGrad\n",
    "$$\\mathbf{h}\\leftarrow \\mathbf{h}+\\frac{\\partial L}{\\partial \\mathbf{W}}\\odot \\frac{\\partial L}{\\partial \\mathbf{W}}$$\n",
    "$$\\mathbf{W} \\leftarrow \\mathbf{W}-\\eta \\frac{1}{\\sqrt{\\mathbf{h}}} \\frac{\\partial L}{\\partial \\mathbf{W}}$$\n",
    "$\\odot$ : 행렬의 원소별 곱셈<br>\n",
    "$\\mathbf{h}$ : 기존 기울기 값을 제곱해 계속 더해줌 (크게 갱신된 원소는 학습률이 낮아짐)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr=lr\n",
    "        self.h=None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h={}\n",
    "            for key, val in params.items():\n",
    "                self.h[key]=np.zeros_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.h[key]+=grads[key]*grads[key]\n",
    "            params[key]-=self.lr*grads[key]/(np.sqrt(self.h[key])+1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ③ Adam = 모멘텀 + AdaGrad\n",
    "<br><br><br>\n",
    "$$f(x,y)=\\frac{1}{20}x^{2}+y^{2}$$\n",
    "![그림6-8](https://raw.githubusercontent.com/asd93/a/master/p/fig%206-8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 가중치의 초깃값\n",
    "초깃값을 모두 0으로 하면 오차역전파법에서 모든 가중치의 값이 똑같이 갱신, 갱신을 거쳐도 같은 값 유지<br>\n",
    "무작위로 설정해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치의 초깃값에 따라 은닉층 활성화값들이 어떻게 변하는지\n",
    "# 활성화함수로 시그모이드 함수 사용하는 5층 신경망\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "x=np.random.randn(1000,100) # 1000개 데이터\n",
    "node_num=100 # 각 은닉층 노드 수 \n",
    "hidden_layer_size=5 # 은닉층 수\n",
    "activations={} # 활성화값 저장\n",
    "\n",
    "for i in range(hidden_layer_size):\n",
    "    if i != 0:\n",
    "        x=activations[i-1]\n",
    "        \n",
    "    w=np.random.randn(node_num, node_num)*1    # 가중치 표준편차 1\n",
    "    a=np.dot(x,w)\n",
    "    z=sigmoid(a)\n",
    "    activations[i]=z\n",
    "    \n",
    "# 히스토그램 그리기\n",
    "\n",
    "for i, a in activations.items():\n",
    "    plt.subplot(1,len(activations), i+1)\n",
    "    plt.title(str(i+1)+\"-layer\")\n",
    "    plt.hist(a.flatten(), 30, range(0,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시그모이드 함수는 출력값이 0이나 1에 가까워지면 미분이 0으로 다가감.<br>\n",
    "데이터가 0과 1에 치우쳐 분포하게 되면 역전파 기울기 값이 작아지다가 사라짐 - **기울기 소실**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.random.randn(1000,100) # 1000개 데이터\n",
    "node_num=100 # 각 은닉층 노드 수 \n",
    "hidden_layer_size=5 # 은닉층 수\n",
    "activations={} # 활성화값 저장\n",
    "\n",
    "for i in range(hidden_layer_size):\n",
    "    if i != 0:\n",
    "        x=activations[i-1]\n",
    "        \n",
    "    w=np.random.randn(node_num, node_num)/np.sqrt(node_num)    # Xavior 초깃값 - 표준편차가1/n^0.5\n",
    "    a=np.dot(x,w)\n",
    "    z=sigmoid(a)\n",
    "    activations[i]=z\n",
    "    \n",
    "    \n",
    "for i, a in activations.items():\n",
    "    plt.subplot(1,len(activations), i+1)\n",
    "    plt.title(str(i+1)+\"-layer\")\n",
    "    plt.hist(a.flatten(), 30, range(0,1))\n",
    "plt.show()\n",
    "\n",
    "# 넓게 분포"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일그러짐은 sigmoid 대신 tanh 이용하면 개선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid를 사용할 때의 가중치 초깃값\n",
    "**Xavior 초깃값**<br>\n",
    "앞 계층 노드가 n개일 때, 표준편차가 $\\frac{1}{\\sqrt{n}}$ 인 정규분포 사용\n",
    "\n",
    "### ReLU를 사용할 때의 가중치 초깃값\n",
    "**He 초깃값**<br>\n",
    "앞 계층 노드가 n개일 때, 표준편차가 $\\sqrt{\\frac{2}{n}}$ 인 정규분포 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 배치 정규화\n",
    "각 층에서의 활성화 값이 적당히 분포되도록 조정\n",
    "* 학습 속도 개선\n",
    "* 초깃값에 크게 의존하지 않음\n",
    "* 오버피팅 억제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미니배치 단위로 정규화<br>\n",
    "데이터 분포 평균 0, 분산 1 되도록 정규화<br>\n",
    "$$\\mu _{B}\\leftarrow \\frac{1}{m}\\sum_{i=1}^{m}x_{i}$$\n",
    "$$\\sigma _{B} ^{2}\\leftarrow \\frac{1}{m}\\sum_{i=1}^{m}(x_{i}-\\mu_{B})^2$$\n",
    "$$\\hat{x_i}\\leftarrow \\frac{x_i-\\mu_B}{\\sqrt{\\sigma_B^2+\\varepsilon }}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 바른 학습을 위해\n",
    "### 오버피팅\n",
    "* 매개변수가 많고 표현력이 높은 모델\n",
    "* 훈련 데이터가 적음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "x_train=x_train[:300]\n",
    "t_train=t_train[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network=MultiLayerNet(input_size=784, hidden_size_list[100,100,100,100,100,100], output_size=10)\n",
    "optimizer=SGD(lr=0.01)\n",
    "max_epochs=201\n",
    "train_size=x_train.shape[0]\n",
    "batch_size=100\n",
    "\n",
    "train_loss_list=[]\n",
    "train_acc_list=[]\n",
    "test_acc_list=[]\n",
    "\n",
    "iter_per_epoch=max(train_size/batch_size, 1)\n",
    "epoch_cnt=0\n",
    "\n",
    "for i in range(1000000000):\n",
    "    batch_mask=np.random.choice(train_size,batch,size)\n",
    "    x_batch=x_train[batch_mask]\n",
    "    t_batch=t_train[batch_mask]\n",
    "    \n",
    "    grads=network.gradient(x_batch, t_batch)\n",
    "    optimizer.update(network.params, grads)\n",
    "    \n",
    "    if i % iter_per_epoch ==0:\n",
    "        train_acc=network.accuracy(x_train, t_train)\n",
    "        test_acc=network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        \n",
    "        epoch_cnt += 1\n",
    "        if epoch_cnt >= max_epochs:\n",
    "            break\n",
    "            \n",
    "# train_acc_list 와 test_acc_list 에 에폭 단위의 정확도 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![그림 6-20](https://raw.githubusercontent.com/asd93/a/master/p/fig%206-20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가중치 감소\n",
    "큰 가중치에 대해서 그에 상응하는 큰 페널티를 부과하여 오버피팅을 억제<br><br>\n",
    "모든 가중치 각각의 손실함수에 $\\frac{1}{2}\\lambda \\mathbf{W}^2$ 를 더함<BR>\n",
    "$\\lambda$ : 가중치 세기 조절하는 하이퍼파라미터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 드롭아웃\n",
    "훈련 때 은닉층의 뉴런을 무작위로 골라 삭제<br>\n",
    "시험 때는 모든 뉴런에 신호를 전달하고 각 뉴런의 출력에 훈련 때 삭제 안 한 비율을 곱하여 출력<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self,dropout_ratio=0.5):\n",
    "        self.dropout_ratio+dropout_ratio\n",
    "        self.mask=None\n",
    "        \n",
    "    def forward(self, x, train_fig=True):\n",
    "        if train_fig:\n",
    "            self.mask=np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            return x*self.mask\n",
    "        else:\n",
    "            return x+(1.0-self.dropout_ratio)\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        return dout*self.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 적절한 하이퍼파라미터 값 찾기\n",
    "### 검증 데이터\n",
    "* 훈련 데이터:매개변수 학습\n",
    "* 검증 데이터:하이퍼파라미터 성능 평가\n",
    "* 시험 데이터:신경망의 범용 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, t_train),(x_test,t_test)=load_mnist()\n",
    "\n",
    "# 훈련 데이터를 뒤섞는다.\n",
    "x_train, t_train= shuffle_dataset(x_train, t_train)\n",
    "\n",
    "# 20%를 검증 데이터로 분할\n",
    "validation_rate=0.20\n",
    "validation_num=int(x_train.shape[0]*validation_rate)\n",
    "\n",
    "x_val=x_train[:validation_num]\n",
    "t_val=t_train[:validation_num]\n",
    "x_train=x_train[validation_num:]\n",
    "t_train=t_train[validation_num:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
