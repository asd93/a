{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "ch4_17101605.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asd93/a/blob/master/ch4_17101605.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8zpmFt7tuyL",
        "colab_type": "text"
      },
      "source": [
        "# 4.3 수치미분"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLSKpcCVtuyX",
        "colab_type": "text"
      },
      "source": [
        "## 4.3.1 미분"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9heHRgqtuyc",
        "colab_type": "text"
      },
      "source": [
        "$\\frac{df(x)}{dx}=\\lim_{h\\rightarrow 0}\\frac{f(x+h)-f(x)}{h}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mpwBSQytuyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 나쁜 구현 예\n",
        "def numerical_diff(f,x):\n",
        "    h=10e-50\n",
        "    return (f(x+h)-f(x))/h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpYPpzSHtuy6",
        "colab_type": "code",
        "colab": {},
        "outputId": "aae6d3af-a3cc-4578-ea1e-de4492306b48"
      },
      "source": [
        "# 1. 작은 값이 생략되어 최종 계산 결과에 오차가 생긴다\n",
        "# 2. x위치의 함수의 기울기가 아니라 (x+h)와 x 사이의 기울기에 해당한다\n",
        "\n",
        "import numpy as np\n",
        "np.float32(1e-50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNwiDXEbtuzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 중심 차분, 중앙 차분\n",
        "# 수치 미분 - 작은 차분으로 미분 계산\n",
        "def numerical_diff(f,x):\n",
        "    h=1e-4 #0.0001\n",
        "    return (f(x+h)-f(x-h)) / 2*h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SxU6CKqtuzf",
        "colab_type": "text"
      },
      "source": [
        "## 4.3.2 수치 미분의 예"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6EkXBZrtuzl",
        "colab_type": "text"
      },
      "source": [
        "$y=0.01x^{2}+0.1x$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPHuQ535tuzp",
        "colab_type": "code",
        "colab": {},
        "outputId": "db73f526-0118-46fa-f746-1dbfd57f78ad"
      },
      "source": [
        "def function_1(x):\n",
        "    return 0.01*x**2 + 0.1*x\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "x=np.arange(0.0,20.0,0.1) #0에서 20까지 0.1 간격의 배열 x를 만든다 (20은 미포함)\n",
        "y=function_1(x)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.plot(x,y)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOfP4e9etuz4",
        "colab_type": "code",
        "colab": {},
        "outputId": "e65191d9-c165-4544-cd44-31457d087d48"
      },
      "source": [
        "#x=5\n",
        "numerical_diff(function_1,5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.9999999999908982e-09"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6504oYe5tu0K",
        "colab_type": "code",
        "colab": {},
        "outputId": "fb8c9d90-f503-4d47-9e79-027fd324e211"
      },
      "source": [
        "#x=10\n",
        "numerical_diff(function_1,10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.999999999986347e-09"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jej6z8Z3tu0c",
        "colab_type": "text"
      },
      "source": [
        "$f(x)=0.01x^{2}+0.1x$<br><br>\n",
        "$\\frac{df(x)}{dx}=0.02x+0.1$<br><br>\n",
        "x=5 f'(x)=0.2<br>\n",
        "x=10 f'(x)=0.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfWTIJ-etu0g",
        "colab_type": "text"
      },
      "source": [
        "## 4.3.3 편미분"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAnDRJLitu0k",
        "colab_type": "text"
      },
      "source": [
        "$f(x_{0},x_{1})=x_{0}^{2}+x_{1}^{2}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cx2TtP4rtu0p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def function_2(x):\n",
        "    return x[0]**2 + x[1]**2\n",
        "    # 또는 return np.sum(x**2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYqp45CBtu09",
        "colab_type": "text"
      },
      "source": [
        "문제 1 : $x_{0}=3, x_{1}=4$일 때, $x_{0}$에 대한 편미분 $\\frac{\\partial f}{\\partial x_{0}}$를 구하라."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83G-CLxdtu1E",
        "colab_type": "code",
        "colab": {},
        "outputId": "286199cb-0c5c-456b-f4cf-9534c98f240a"
      },
      "source": [
        "def function_tmp1(x0):\n",
        "    return x0*x0 + 4.0**2.0\n",
        "\n",
        "numerical_diff(function_tmp1,3.0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.000000000003781e-08"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IDiDtXdtu1U",
        "colab_type": "text"
      },
      "source": [
        "문제 2 : $x_{0}=3, x_{1}=4$일 때, $x_{1}$에 대한 편미분 $\\frac{\\partial f}{\\partial x_{1}}$를 구하라."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gps6aEnttu1Y",
        "colab_type": "code",
        "colab": {},
        "outputId": "81974508-81dc-4b25-bf17-c10fb467b3bc"
      },
      "source": [
        "def function_tmp2(x1):\n",
        "    return 3.0**2.0 + x1*x1\n",
        "\n",
        "numerical_diff(function_tmp2,4.0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.999999999999119e-08"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tx-NDNbCtu1s",
        "colab_type": "text"
      },
      "source": [
        "# 4.4 기울기\n",
        "##### 모든 변수의 편미분을 벡터로 정리한 것"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNfgX6nmtu1x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def numerical_gradient(f,x):\n",
        "    h=1e-4 #0.0001\n",
        "    grad = np.zeros_like(x) # x와 형상이 같고 원소가 모두 0인 배열을 형성\n",
        "    \n",
        "    for idx in range(x.size):\n",
        "        tmp_val = x[idx]\n",
        "        # f(x+h) 계산\n",
        "        x[idx] = tmp_val + h\n",
        "        fxh1=f(x)\n",
        "        \n",
        "        # f(x-h) 계산\n",
        "        x[idx] = tmp_val - h\n",
        "        fxh2=f(x)\n",
        "        \n",
        "        grad[idx] = (fxh1-fxh2)/(2*h)\n",
        "        x[idx] = tmp_val # 값 복원\n",
        "        \n",
        "    return grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b12JS0H4tu18",
        "colab_type": "code",
        "colab": {},
        "outputId": "5b976e3b-ec8c-4156-b142-421686b9c72a"
      },
      "source": [
        "numerical_gradient(function_2,np.array([3.0,4.0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([6., 8.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1PtEVmktu2H",
        "colab_type": "code",
        "colab": {},
        "outputId": "97f5bb95-8c37-4429-bc28-b0caa26ad92a"
      },
      "source": [
        "numerical_gradient(function_2,np.array([0.0,2.0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 4.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1V9i_iqtu2S",
        "colab_type": "code",
        "colab": {},
        "outputId": "8adce890-9cf2-452d-b495-b556632aa69b"
      },
      "source": [
        "numerical_gradient(function_2,np.array([3.0,0.0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([6., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia8uFOSjtu2e",
        "colab_type": "text"
      },
      "source": [
        "기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향이다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6n6MQE_tu2h",
        "colab_type": "text"
      },
      "source": [
        "## 4.4.1 경사법(경사 하강법)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm-aL3uUtu2k",
        "colab_type": "text"
      },
      "source": [
        "신경망은 학습 시에최적의 매개변수(손실함수가 최솟값이 될 때의 값)를 찾는다. 하지만 일반적으로 손실 함수는 매우 복잡하므로 어디가 최솟값이 되는 곳인지 알 수 없는데 이 때 기울기를 이용해 함수의 최솟값을 찾는것이 경사법이다.<br>\n",
        "함수의 한 위치에서 기울어진 방향으로 일정 거리만큼 이동한다. 그런 다음 이동한 곳에서도 마찬가지로 기울기를 구하고, 또 그 기울어진 방향으로 나아가기를 반복한다. 이렇게 해서 함수의 값을 점차 줄이는 것이 경사법이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS4bAaTotu2n",
        "colab_type": "text"
      },
      "source": [
        "$x_{0}=x_{0}-\\eta \\frac{\\partial f}{\\partial x_{0}}$<br>\n",
        "$x_{1}=x_{1}-\\eta \\frac{\\partial f}{\\partial x_{1}}$<br> \n",
        "$\\eta $ : 학습률 - 매개변수 값을 얼마나 갱신하느냐를 정하는 것 (0.01,0.001 등 미리 특정 값으로 정해두어야 함)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cA1SmSHtu2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
        "    x=init_x\n",
        "    \n",
        "    for i in range(step_num):\n",
        "        grad=numerical_gradient(f,x)\n",
        "        x -= lr*grad\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5ziS85ltu22",
        "colab_type": "text"
      },
      "source": [
        "f : 최적화하려는 함수<br>init_x : 초깃값<br>lr : learning rate,학습률<br>step_num : 경사법 반복 횟수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWv_vFs4tu26",
        "colab_type": "text"
      },
      "source": [
        "문제 : 경사법으로 $f(x_{0},x_{1})=x_{0}^{2}+x_{1}^{2}$의 최솟값을 구하라."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu3US9pbtu29",
        "colab_type": "code",
        "colab": {},
        "outputId": "150930de-e490-44bf-8220-bd4f82e67379"
      },
      "source": [
        "def function_2(x):\n",
        "    return x[0]**2 + x[1]**2\n",
        "\n",
        "init_x=np.array([-3.0,4.0])\n",
        "gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-6.11110793e-10,  8.14814391e-10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOPgFkYstu3Q",
        "colab_type": "code",
        "colab": {},
        "outputId": "31a566e7-f2af-4a8e-90e7-cb60be626ded"
      },
      "source": [
        "#학습률이 너무 큰 예 : lr=10.0\n",
        "init_x=np.array([-3.0,4.0])\n",
        "gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100)\n",
        "\n",
        "#큰 값으로 발산"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-2.58983747e+13, -1.29524862e+12])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WSDezdQtu3n",
        "colab_type": "code",
        "colab": {},
        "outputId": "bf2c7d97-ea52-4e06-9da0-61da486e547b"
      },
      "source": [
        "#학습률이 너무 작은 예 : lr=1e-10\n",
        "init_x=np.array([-3.0,4.0])\n",
        "gradient_descent(function_2, init_x=init_x, lr=1e-10, step_num=100)\n",
        "\n",
        "#거의 갱신되지 않음"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-2.99999994,  3.99999992])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuMr7Mgptu30",
        "colab_type": "text"
      },
      "source": [
        "가중치,편향 : 훈련 데이터와 학습 알고리즘에 의해서 자동으로 획득됨<br>\n",
        "학습률 : 사람이 직접 설정해야 함 (하이퍼파라미터)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiOuNSXStu34",
        "colab_type": "text"
      },
      "source": [
        "## 4.4.2 신경망에서의 기울기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC2uqjbztu38",
        "colab_type": "text"
      },
      "source": [
        "형상이 $2\\times 3$, 가중치가 $\\mathbf{W}$, 손실함수가 L인 신경망<br>경사 $\\frac{\\partial L}{\\partial \\mathbf{W}}$<br>\n",
        "$\\frac{\\partial L}{\\partial \\mathbf{W}}$의 형상은 $\\mathbf{W}$와 같다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNBZ1ywVtu4A",
        "colab_type": "text"
      },
      "source": [
        "$\\mathbf{W}=\\bigl(\\begin{smallmatrix}\n",
        "w_{11} & w_{12} & w_{13}\\\\ \n",
        "w_{21} & w_{22} & w_{23}\n",
        "\\end{smallmatrix}\\bigr)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlOs7pDTtu4F",
        "colab_type": "text"
      },
      "source": [
        "### $\\frac{\\partial L}{\\partial \\mathbf{W}}=\\bigl(\\begin{smallmatrix}\n",
        "\\frac{\\partial L}{\\partial w_{11}} & \\frac{\\partial L}{\\partial w_{12}} & \\frac{\\partial L}{\\partial w_{13}}\\\\ \n",
        "\\frac{\\partial L}{\\partial w_{21}} & \\frac{\\partial L}{\\partial w_{22}} & \\frac{\\partial L}{\\partial w_{23}}\n",
        "\\end{smallmatrix}\\bigr)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTHTWqqxtu4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys, os\n",
        "sys.path.append(r\"C:\\Users\\이유선\\Desktop\\'▽'\\py\\deep-learning-from-scratch-master\")\n",
        "import numpy as np\n",
        "from common.functions import softmax, cross_entropy_error\n",
        "from common.gradient import numerical_gradient\n",
        "\n",
        "class simpleNet:\n",
        "    def __init__(self):\n",
        "        self.W=np.random.randn(2,3) #정규분포로 초기화\n",
        "        \n",
        "    def predict(self,x):\n",
        "        return np.dot(x, self.W)\n",
        "    \n",
        "    def loss(self,x,t): # x 입력데이터 t 정답레이블\n",
        "        z=self.predict(x)\n",
        "        y=softmax(z)\n",
        "        loss=cross_entropy_error(y,t)\n",
        "        \n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pns3mylJtu4W",
        "colab_type": "code",
        "colab": {},
        "outputId": "8980c36e-18a1-48b7-80ed-5291bcb2fa35"
      },
      "source": [
        "net=simpleNet()\n",
        "print(net.W) #가중치 매개변수"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.28784646 -0.55651218 -0.25385907]\n",
            " [ 1.38540908 -2.10760105 -0.06095728]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQn0UwAWtu4g",
        "colab_type": "code",
        "colab": {},
        "outputId": "179f2696-add4-4489-ff2b-41697fa84a0b"
      },
      "source": [
        "x=np.array([0.6,0.9])\n",
        "p=net.predict(x)\n",
        "print(p)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1.0741603  -2.23074826 -0.20717699]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJI-LzDktu4q",
        "colab_type": "code",
        "colab": {},
        "outputId": "7d368713-5420-4a0e-ef2d-503e8a9503c0"
      },
      "source": [
        "np.argmax(p) #최댓값의 인덱스"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJnbTUV-tu41",
        "colab_type": "code",
        "colab": {},
        "outputId": "4117513c-3695-42e7-de44-cb4b76f822eb"
      },
      "source": [
        "t=np.array([0,0,1]) #정답 레이블\n",
        "net.loss(x,t)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.5546929843563622"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6LRSu9Btu4-",
        "colab_type": "code",
        "colab": {},
        "outputId": "964f4788-3138-4178-b1e3-c0d42551aefb"
      },
      "source": [
        "def f(W):\n",
        "    return net.loss(x,t)\n",
        "\n",
        "dW= numerical_gradient(f, net.W)\n",
        "print(dW)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.45649284  0.01675446 -0.4732473 ]\n",
            " [ 0.68473926  0.02513169 -0.70987095]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCD9eqXutu5J",
        "colab_type": "text"
      },
      "source": [
        "$\\frac{\\partial L}{\\partial w_{11}}$은 음의 방향으로 갱신하고, $\\frac{\\partial L}{\\partial w_{23}}$ 은 양의 방향으로 갱신해야 한다.<br>\n",
        "한 번에 갱신되는 양에는 $\\frac{\\partial L}{\\partial w_{23}}$이 $\\frac{\\partial L}{\\partial w_{11}}$보다 크게 기여한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFXtK_0Itu5L",
        "colab_type": "text"
      },
      "source": [
        "# 4.5 학습 알고리즘 구현하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kL2lHV0tu5Q",
        "colab_type": "text"
      },
      "source": [
        "신경망 학습의 절차<br><br>\n",
        "\n",
        "    1단계 - 미니배치\n",
        ">훈련 데이터 중 일부를 무작위로 가져옵니다. 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실 함수 값을 줄이는 것이 목표입니다.\n",
        "\n",
        "    2단계 - 기울기 산출\n",
        ">미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구합니다. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시합니다.\n",
        "\n",
        "    3단계 - 매개변수 갱신\n",
        ">가중치 매개변수를 기울기 방향으로 아주 조금 갱신합니다.\n",
        "\n",
        "    4단계 - 반복\n",
        ">1~3단계를 반복합니다.\n",
        "\n",
        "\n",
        "이는 경사 하강법으로 매개변수를 갱신하는 방법이며, 미니배치로 무작위로 선정하기 때문에 **확률적 경사 하강법** 이라고 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3dA3hlstu5T",
        "colab_type": "text"
      },
      "source": [
        "## 4.5.1 2층 신경망 클래스 구현하기\n",
        "#### 2층 신경망(은닉층 1개)을 대상으로 MNIST 데이터셋을 사용한다<br>일단 2층 신경망을 하나의 클래스로 구현한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeZYwvdhtu5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys, os\n",
        "sys.path.append(r\"C:\\Users\\이유선\\Desktop\\'▽'\\py\\deep-learning-from-scratch-master\")\n",
        "from common.functions import *\n",
        "from common.gradient import numerical_gradient\n",
        "\n",
        "class TwoLayerNet:\n",
        "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
        "        #가중치 초기화\n",
        "        self.params={}\n",
        "        self.params['W1']=weight_init_std * \\\n",
        "                          np.random.randn(input_size,hidden_size)\n",
        "        self.params['b1']=np.zeros(hidden_size)\n",
        "        self.params['W2']=weight_init_std * \\\n",
        "                          np.random.randn(hidden_size,output_size)\n",
        "        self.params['b2']=np.zeros(output_size)\n",
        "        \n",
        "    def predict(self,x):\n",
        "        W1,W2=self.params['W1'],self.params['W2']\n",
        "        b1,b2=self.params['b1'],se;f.params['b2']\n",
        "        \n",
        "        a1=np.dot(x,W1)+b1\n",
        "        z1=sigmoid(a1)\n",
        "        a2=np.dot(z1,W2)+b2\n",
        "        y=softmax(a2)\n",
        "        \n",
        "        return y\n",
        "    \n",
        "    # x:입력 데이터, t:정답 레이블\n",
        "    def loss(self,x,t):\n",
        "        y=self.predict(x)\n",
        "        \n",
        "        return cross_entropy_error(y,t)\n",
        "    \n",
        "    def accuracy(self,x,t):\n",
        "        y=self.predict(x)\n",
        "        y=np.argmax(y,axis=1)\n",
        "        t=np.argmax(t,axis=1)\n",
        "        \n",
        "        accuracy=np.sum(y==t)/float(x.shape[0])\n",
        "        return accuracy\n",
        "    \n",
        "    # x:입력 데이터, t:정답 레이블\n",
        "    def numerical_gradient(self,x,t):\n",
        "        loss_W=lambda W:self.loss(x,t)\n",
        "        \n",
        "        grads={}\n",
        "        grads['W1']=numerical_gradient(loss_W,self.params['W1'])\n",
        "        grads['b1']=numerical_gradient(loss_W,self.params['b1'])\n",
        "        grads['W2']=numerical_gradient(loss_W,self.params['W2'])\n",
        "        grads['b2']=numerical_gradient(loss_W,self.params['b2'])\n",
        "        \n",
        "        return grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_1lyhY5tu5n",
        "colab_type": "text"
      },
      "source": [
        "변수 | 설명\n",
        "---- | ----\n",
        "params | 신경망의 매개변수를 보관하는 딕셔너리 변수(인스턴스 변수)<br>params['W1']은 1번째 층의 가중치, params['b1']은 1번째 층의 편향<br>params['W2']는 2번째 층의 가중치, params['b2']는 2번째 층의 편향\n",
        "grads | 기울기 보관하는 딕셔너리 변수(numerical_gradient() 메서드의 반환 값)<br>grad['W1']은 1번째 층의 가중치의 기울기, grad['b1']은 1번째 층의 편향의 기울기<br>grad['W2']는 2번째 층의 가중치의 기울기, grad['b2']는 2번째 층의 편향의 기울기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQsf3FsGtu5r",
        "colab_type": "text"
      },
      "source": [
        "변수 | 설명\n",
        "---- | ----\n",
        "__init__(self,input_size,hidden_size,output_size) | 초기화를 수행한다. <br>인수는 순서대로 입력층의 뉴런 수, 은닉층의 뉴런 수, 출력층의 뉴런 수\n",
        "predict(self,x) | 예측(추론)을 수행한다.<br>인수 x는 이미지 데이터\n",
        "loss(self,x,t) | 손실함수의 값을 구한다.<br>인수 x는 이미지 데이터, t는 정답 레이블(아래 칸의 세 메서드의 인수들도 마찬가지)\n",
        "accuracy(self,x,t) | 정확도를 구한다.\n",
        "numerical_gradient(self,x,t) | 가중치 매개변수의 기울기를 구한다.\n",
        "gradient(self,x,t) | 가중치 매개변수의 기울기를 구한다.<br>numerical_gradient()의 성능 개선판"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGpfhYB6tu5v",
        "colab_type": "text"
      },
      "source": [
        "## 4.5.2 미니배치 학습 구현하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMVMiVXOtu5y",
        "colab_type": "text"
      },
      "source": [
        "훈련 데이터 일부를 무작위로 꺼내고 그 미니배치에 대해서 경사법으로 매개변수 갱신"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5upgBWGmtu51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "sys.path.append(r\"C:\\Users\\이유선\\Desktop\\'▽'\\py\\deep-learning-from-scratch-master\\ch04\")\n",
        "from dataset.mnist import load_mnist\n",
        "from two_layer_net import TwoLayerNet\n",
        "\n",
        "(x_train,t_train),(x_test,t_test)=\\\n",
        "    load_mnist(normalize=True,one_hot_label=True)\n",
        "\n",
        "train_loss_list=[]\n",
        "\n",
        "#하이퍼파라미터\n",
        "iters_num=10000 #반복횟수\n",
        "train_size=x_train.shape[0]\n",
        "batch_size=100 #미니배치 크기\n",
        "learning_rate=0.1\n",
        "\n",
        "network=TwoLayerNet(input_size=784,hidden_size=50,output_size=10)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    #미니배치 획득\n",
        "    batch_mask=np.random.choice(train_size,batch_size)\n",
        "    x_batch=x_train[batch_mask]\n",
        "    t_batch=t_train[batch_mask]\n",
        "    \n",
        "    #기울기 계산\n",
        "    grad=network.numerical_gradient(x_batch,t_batch)\n",
        "    #grad=network.gradient(x_batch,t_batch) #성능 개선판\n",
        "    \n",
        "    #매개변수 갱신\n",
        "    for key in ('W1','b1','W2','b2'):\n",
        "        network.params[key]-=learning_rate * grad[key]\n",
        "    \n",
        "    #학습 경과 기록\n",
        "    loss=network.loss(x_batch,t_batch)\n",
        "    train_loss_list.append(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkSwClKctu5-",
        "colab_type": "text"
      },
      "source": [
        "미니배치 크기를 100으로 하고 그 100개의 미니배치를 대상으로 확률적 경사 하강법을 수행해 매개변수를 갱신한다.<br>\n",
        "경사법에 의한 갱신 횟수를 10000번으로 설정하고, 갱신할 때마다 손실 함수를 계산하고 그 값을 배열에 추가한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFIlvj9vtu6B",
        "colab_type": "text"
      },
      "source": [
        "## 4.5.3 시험 데이터로 평가하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbnM_-Yctu6E",
        "colab_type": "text"
      },
      "source": [
        "'오버피팅'을 일으키는지 확인<br>\n",
        "학습 도중 정기적으로 훈련 데이터와 시험 데이터를 대상으로 정확도를 기록<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkrFOkTDtu6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "sys.path.append(r\"C:\\Users\\이유선\\Desktop\\'▽'\\py\\deep-learning-from-scratch-master\\ch04\")\n",
        "from dataset.mnist import load_mnist\n",
        "from two_layer_net import TwoLayerNet\n",
        "\n",
        "(x_train,t_train),(x_test,t_test)=\\\n",
        "    load_mnist(normalize=True,one_hot_label=True)\n",
        "\n",
        "network=TwoLayerNet(input_size=784,hidden_size=50,output_size=10)\n",
        "\n",
        "#하이퍼파라미터\n",
        "iters_num=10000 #반복횟수\n",
        "train_size=x_train.shape[0]\n",
        "batch_size=100 #미니배치 크기\n",
        "learning_rate=0.1\n",
        "\n",
        "train_loss_list=[]\n",
        "train_acc_list=[]\n",
        "test_acc_lis=[]\n",
        "\n",
        "#1에폭당 반복 수\n",
        "iter_per_epoch=max(train_size/batch_size,1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    #미니배치 획득\n",
        "    batch_mask=np.random.choice(train_size,batch_size)\n",
        "    x_batch=x_train[batch_mask]\n",
        "    t_batch=t_train[batch_mask]\n",
        "    \n",
        "    #기울기 계산\n",
        "    grad=network.numerical_gradient(x_batch,t_batch)\n",
        "    #grad=network.gradient(x_batch,t_batch) #성능 개선판\n",
        "    \n",
        "    #매개변수 갱신\n",
        "    for key in ('W1','b1','W2','b2'):\n",
        "        network.params[key]-=learning_rate * grad[key]\n",
        "    \n",
        "    #학습 경과 기록\n",
        "    loss=network.loss(x_batch,t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "    \n",
        "    #1에폭당 정확도 계산\n",
        "    if i% iter_per_epoch ==0:\n",
        "        train_acc=network.accuracy(x_train,t_train)\n",
        "        test_acc=network.accuract(x_text,t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(\"train acc, test acc :\" + str(train_acc) + \",\" + str(test_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}