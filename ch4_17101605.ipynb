{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 수치미분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.1 미분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{df(x)}{dx}=\\lim_{h\\rightarrow 0}\\frac{f(x+h)-f(x)}{h}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나쁜 구현 예\n",
    "def numerical_diff(f,x):\n",
    "    h=10e-50\n",
    "    return (f(x+h)-f(x))/h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 작은 값이 생략되어 최종 계산 결과에 오차가 생긴다\n",
    "# 2. x위치의 함수의 기울기가 아니라 (x+h)와 x 사이의 기울기에 해당한다\n",
    "\n",
    "import numpy as np\n",
    "np.float32(1e-50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중심 차분, 중앙 차분\n",
    "# 수치 미분 - 작은 차분으로 미분 계산\n",
    "def numerical_diff(f,x):\n",
    "    h=1e-4 #0.0001\n",
    "    return (f(x+h)-f(x-h)) / 2*h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.2 수치 미분의 예"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y=0.01x^{2}+0.1x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "x=np.arange(0.0,20.0,0.1) #0에서 20까지 0.1 간격의 배열 x를 만든다 (20은 미포함)\n",
    "y=function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9999999999908982e-09"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x=5\n",
    "numerical_diff(function_1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.999999999986347e-09"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x=10\n",
    "numerical_diff(function_1,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x)=0.01x^{2}+0.1x$<br><br>\n",
    "$\\frac{df(x)}{dx}=0.02x+0.1$<br><br>\n",
    "x=5 f'(x)=0.2<br>\n",
    "x=10 f'(x)=0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.3 편미분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x_{0},x_{1})=x_{0}^{2}+x_{1}^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "    # 또는 return np.sum(x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문제 1 : $x_{0}=3, x_{1}=4$일 때, $x_{0}$에 대한 편미분 $\\frac{\\partial f}{\\partial x_{0}}$를 구하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.000000000003781e-08"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_tmp1(x0):\n",
    "    return x0*x0 + 4.0**2.0\n",
    "\n",
    "numerical_diff(function_tmp1,3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문제 2 : $x_{0}=3, x_{1}=4$일 때, $x_{1}$에 대한 편미분 $\\frac{\\partial f}{\\partial x_{1}}$를 구하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119e-08"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_tmp2(x1):\n",
    "    return 3.0**2.0 + x1*x1\n",
    "\n",
    "numerical_diff(function_tmp2,4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 기울기\n",
    "##### 모든 변수의 편미분을 벡터로 정리한 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f,x):\n",
    "    h=1e-4 #0.0001\n",
    "    grad = np.zeros_like(x) # x와 형상이 같고 원소가 모두 0인 배열을 형성\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        # f(x+h) 계산\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1=f(x)\n",
    "        \n",
    "        # f(x-h) 계산\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2=f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1-fxh2)/(2*h)\n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 8.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2,np.array([3.0,4.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 4.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2,np.array([0.0,2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2,np.array([3.0,0.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향이다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4.1 경사법(경사 하강법)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망은 학습 시에최적의 매개변수(손실함수가 최솟값이 될 때의 값)를 찾는다. 하지만 일반적으로 손실 함수는 매우 복잡하므로 어디가 최솟값이 되는 곳인지 알 수 없는데 이 때 기울기를 이용해 함수의 최솟값을 찾는것이 경사법이다.<br>\n",
    "함수의 한 위치에서 기울어진 방향으로 일정 거리만큼 이동한다. 그런 다음 이동한 곳에서도 마찬가지로 기울기를 구하고, 또 그 기울어진 방향으로 나아가기를 반복한다. 이렇게 해서 함수의 값을 점차 줄이는 것이 경사법이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_{0}=x_{0}-\\eta \\frac{\\partial f}{\\partial x_{0}}$<br>\n",
    "$x_{1}=x_{1}-\\eta \\frac{\\partial f}{\\partial x_{1}}$<br> \n",
    "$\\eta $ : 학습률 - 매개변수 값을 얼마나 갱신하느냐를 정하는 것 (0.01,0.001 등 미리 특정 값으로 정해두어야 함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x=init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad=numerical_gradient(f,x)\n",
    "        x -= lr*grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f : 최적화하려는 함수<br>init_x : 초깃값<br>lr : learning rate,학습률<br>step_num : 경사법 반복 횟수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문제 : 경사법으로 $f(x_{0},x_{1})=x_{0}^{2}+x_{1}^{2}$의 최솟값을 구하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x=np.array([-3.0,4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.58983747e+13, -1.29524862e+12])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#학습률이 너무 큰 예 : lr=10.0\n",
    "init_x=np.array([-3.0,4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100)\n",
    "\n",
    "#큰 값으로 발산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.99999994,  3.99999992])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#학습률이 너무 작은 예 : lr=1e-10\n",
    "init_x=np.array([-3.0,4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=1e-10, step_num=100)\n",
    "\n",
    "#거의 갱신되지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가중치,편향 : 훈련 데이터와 학습 알고리즘에 의해서 자동으로 획득됨<br>\n",
    "학습률 : 사람이 직접 설정해야 함 (하이퍼파라미터)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4.2 신경망에서의 기울기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "형상이 $2\\times 3$, 가중치가 $\\mathbf{W}$, 손실함수가 L인 신경망<br>경사 $\\frac{\\partial L}{\\partial \\mathbf{W}}$<br>\n",
    "$\\frac{\\partial L}{\\partial \\mathbf{W}}$의 형상은 $\\mathbf{W}$와 같다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{W}=\\bigl(\\begin{smallmatrix}\n",
    "w_{11} & w_{12} & w_{13}\\\\ \n",
    "w_{21} & w_{22} & w_{23}\n",
    "\\end{smallmatrix}\\bigr)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\frac{\\partial L}{\\partial \\mathbf{W}}=\\bigl(\\begin{smallmatrix}\n",
    "\\frac{\\partial L}{\\partial w_{11}} & \\frac{\\partial L}{\\partial w_{12}} & \\frac{\\partial L}{\\partial w_{13}}\\\\ \n",
    "\\frac{\\partial L}{\\partial w_{21}} & \\frac{\\partial L}{\\partial w_{22}} & \\frac{\\partial L}{\\partial w_{23}}\n",
    "\\end{smallmatrix}\\bigr)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(r\"C:\\Users\\이유선\\Desktop\\'▽'\\py\\deep-learning-from-scratch-master\")\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W=np.random.randn(2,3) #정규분포로 초기화\n",
    "        \n",
    "    def predict(self,x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self,x,t): # x 입력데이터 t 정답레이블\n",
    "        z=self.predict(x)\n",
    "        y=softmax(z)\n",
    "        loss=cross_entropy_error(y,t)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.28784646 -0.55651218 -0.25385907]\n",
      " [ 1.38540908 -2.10760105 -0.06095728]]\n"
     ]
    }
   ],
   "source": [
    "net=simpleNet()\n",
    "print(net.W) #가중치 매개변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.0741603  -2.23074826 -0.20717699]\n"
     ]
    }
   ],
   "source": [
    "x=np.array([0.6,0.9])\n",
    "p=net.predict(x)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p) #최댓값의 인덱스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5546929843563622"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=np.array([0,0,1]) #정답 레이블\n",
    "net.loss(x,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.45649284  0.01675446 -0.4732473 ]\n",
      " [ 0.68473926  0.02513169 -0.70987095]]\n"
     ]
    }
   ],
   "source": [
    "def f(W):\n",
    "    return net.loss(x,t)\n",
    "\n",
    "dW= numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial L}{\\partial w_{11}}$은 음의 방향으로 갱신하고, $\\frac{\\partial L}{\\partial w_{23}}$ 은 양의 방향으로 갱신해야 한다.<br>\n",
    "한 번에 갱신되는 양에는 $\\frac{\\partial L}{\\partial w_{23}}$이 $\\frac{\\partial L}{\\partial w_{11}}$보다 크게 기여한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5 학습 알고리즘 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망 학습의 절차<br><br>\n",
    "\n",
    "    1단계 - 미니배치\n",
    ">훈련 데이터 중 일부를 무작위로 가져옵니다. 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실 함수 값을 줄이는 것이 목표입니다.\n",
    "\n",
    "    2단계 - 기울기 산출\n",
    ">미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구합니다. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시합니다.\n",
    "\n",
    "    3단계 - 매개변수 갱신\n",
    ">가중치 매개변수를 기울기 방향으로 아주 조금 갱신합니다.\n",
    "\n",
    "    4단계 - 반복\n",
    ">1~3단계를 반복합니다.\n",
    "\n",
    "\n",
    "이는 경사 하강법으로 매개변수를 갱신하는 방법이며, 미니배치로 무작위로 선정하기 때문에 **확률적 경사 하강법** 이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5.1 2층 신경망 클래스 구현하기\n",
    "#### 2층 신경망(은닉층 1개)을 대상으로 MNIST 데이터셋을 사용한다<br>일단 2층 신경망을 하나의 클래스로 구현한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(r\"C:\\Users\\이유선\\Desktop\\'▽'\\py\\deep-learning-from-scratch-master\")\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        #가중치 초기화\n",
    "        self.params={}\n",
    "        self.params['W1']=weight_init_std * \\\n",
    "                          np.random.randn(input_size,hidden_size)\n",
    "        self.params['b1']=np.zeros(hidden_size)\n",
    "        self.params['W2']=weight_init_std * \\\n",
    "                          np.random.randn(hidden_size,output_size)\n",
    "        self.params['b2']=np.zeros(output_size)\n",
    "        \n",
    "    def predict(self,x):\n",
    "        W1,W2=self.params['W1'],self.params['W2']\n",
    "        b1,b2=self.params['b1'],se;f.params['b2']\n",
    "        \n",
    "        a1=np.dot(x,W1)+b1\n",
    "        z1=sigmoid(a1)\n",
    "        a2=np.dot(z1,W2)+b2\n",
    "        y=softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    # x:입력 데이터, t:정답 레이블\n",
    "    def loss(self,x,t):\n",
    "        y=self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y,t)\n",
    "    \n",
    "    def accuracy(self,x,t):\n",
    "        y=self.predict(x)\n",
    "        y=np.argmax(y,axis=1)\n",
    "        t=np.argmax(t,axis=1)\n",
    "        \n",
    "        accuracy=np.sum(y==t)/float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    # x:입력 데이터, t:정답 레이블\n",
    "    def numerical_gradient(self,x,t):\n",
    "        loss_W=lambda W:self.loss(x,t)\n",
    "        \n",
    "        grads={}\n",
    "        grads['W1']=numerical_gradient(loss_W,self.params['W1'])\n",
    "        grads['b1']=numerical_gradient(loss_W,self.params['b1'])\n",
    "        grads['W2']=numerical_gradient(loss_W,self.params['W2'])\n",
    "        grads['b2']=numerical_gradient(loss_W,self.params['b2'])\n",
    "        \n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "변수 | 설명\n",
    "---- | ----\n",
    "params | 신경망의 매개변수를 보관하는 딕셔너리 변수(인스턴스 변수)<br>params['W1']은 1번째 층의 가중치, params['b1']은 1번째 층의 편향<br>params['W2']는 2번째 층의 가중치, params['b2']는 2번째 층의 편향\n",
    "grads | 기울기 보관하는 딕셔너리 변수(numerical_gradient() 메서드의 반환 값)<br>grad['W1']은 1번째 층의 가중치의 기울기, grad['b1']은 1번째 층의 편향의 기울기<br>grad['W2']는 2번째 층의 가중치의 기울기, grad['b2']는 2번째 층의 편향의 기울기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "변수 | 설명\n",
    "---- | ----\n",
    "__init__(self,input_size,hidden_size,output_size) | 초기화를 수행한다. <br>인수는 순서대로 입력층의 뉴런 수, 은닉층의 뉴런 수, 출력층의 뉴런 수\n",
    "predict(self,x) | 예측(추론)을 수행한다.<br>인수 x는 이미지 데이터\n",
    "loss(self,x,t) | 손실함수의 값을 구한다.<br>인수 x는 이미지 데이터, t는 정답 레이블(아래 칸의 세 메서드의 인수들도 마찬가지)\n",
    "accuracy(self,x,t) | 정확도를 구한다.\n",
    "numerical_gradient(self,x,t) | 가중치 매개변수의 기울기를 구한다.\n",
    "gradient(self,x,t) | 가중치 매개변수의 기울기를 구한다.<br>numerical_gradient()의 성능 개선판"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5.2 미니배치 학습 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 데이터 일부를 무작위로 꺼내고 그 미니배치에 대해서 경사법으로 매개변수 갱신"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sys.path.append(r\"C:\\Users\\이유선\\Desktop\\'▽'\\py\\deep-learning-from-scratch-master\\ch04\")\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "(x_train,t_train),(x_test,t_test)=\\\n",
    "    load_mnist(normalize=True,one_hot_label=True)\n",
    "\n",
    "train_loss_list=[]\n",
    "\n",
    "#하이퍼파라미터\n",
    "iters_num=10000 #반복횟수\n",
    "train_size=x_train.shape[0]\n",
    "batch_size=100 #미니배치 크기\n",
    "learning_rate=0.1\n",
    "\n",
    "network=TwoLayerNet(input_size=784,hidden_size=50,output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    #미니배치 획득\n",
    "    batch_mask=np.random.choice(train_size,batch_size)\n",
    "    x_batch=x_train[batch_mask]\n",
    "    t_batch=t_train[batch_mask]\n",
    "    \n",
    "    #기울기 계산\n",
    "    grad=network.numerical_gradient(x_batch,t_batch)\n",
    "    #grad=network.gradient(x_batch,t_batch) #성능 개선판\n",
    "    \n",
    "    #매개변수 갱신\n",
    "    for key in ('W1','b1','W2','b2'):\n",
    "        network.params[key]-=learning_rate * grad[key]\n",
    "    \n",
    "    #학습 경과 기록\n",
    "    loss=network.loss(x_batch,t_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미니배치 크기를 100으로 하고 그 100개의 미니배치를 대상으로 확률적 경사 하강법을 수행해 매개변수를 갱신한다.<br>\n",
    "경사법에 의한 갱신 횟수를 10000번으로 설정하고, 갱신할 때마다 손실 함수를 계산하고 그 값을 배열에 추가한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5.3 시험 데이터로 평가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'오버피팅'을 일으키는지 확인<br>\n",
    "학습 도중 정기적으로 훈련 데이터와 시험 데이터를 대상으로 정확도를 기록<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sys.path.append(r\"C:\\Users\\이유선\\Desktop\\'▽'\\py\\deep-learning-from-scratch-master\\ch04\")\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "(x_train,t_train),(x_test,t_test)=\\\n",
    "    load_mnist(normalize=True,one_hot_label=True)\n",
    "\n",
    "network=TwoLayerNet(input_size=784,hidden_size=50,output_size=10)\n",
    "\n",
    "#하이퍼파라미터\n",
    "iters_num=10000 #반복횟수\n",
    "train_size=x_train.shape[0]\n",
    "batch_size=100 #미니배치 크기\n",
    "learning_rate=0.1\n",
    "\n",
    "train_loss_list=[]\n",
    "train_acc_list=[]\n",
    "test_acc_lis=[]\n",
    "\n",
    "#1에폭당 반복 수\n",
    "iter_per_epoch=max(train_size/batch_size,1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    #미니배치 획득\n",
    "    batch_mask=np.random.choice(train_size,batch_size)\n",
    "    x_batch=x_train[batch_mask]\n",
    "    t_batch=t_train[batch_mask]\n",
    "    \n",
    "    #기울기 계산\n",
    "    grad=network.numerical_gradient(x_batch,t_batch)\n",
    "    #grad=network.gradient(x_batch,t_batch) #성능 개선판\n",
    "    \n",
    "    #매개변수 갱신\n",
    "    for key in ('W1','b1','W2','b2'):\n",
    "        network.params[key]-=learning_rate * grad[key]\n",
    "    \n",
    "    #학습 경과 기록\n",
    "    loss=network.loss(x_batch,t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    #1에폭당 정확도 계산\n",
    "    if i% iter_per_epoch ==0:\n",
    "        train_acc=network.accuracy(x_train,t_train)\n",
    "        test_acc=network.accuract(x_text,t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc :\" + str(train_acc) + \",\" + str(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
